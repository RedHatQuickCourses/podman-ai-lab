= Appendix A

Content for Appendix A...

=== GGUF Format

One of the main advantages of GGUF is that it allows users to run LLMs on their CPU. This is particularly beneficial for users who may not own a powerful GPU or who have limited access to GPU resources.

"GGUF is a file format for storing models for inference with GGML and executors based on GGML. GGUF is a binary format that is designed for fast loading and saving of models, and for ease of reading. Models are traditionally developed using PyTorch or another framework, and then converted to GGUF for use in GGML."


the new GGUF (GPT-Generated Unified Format) framework has been developed to facilitate the operation of Large Language Models (LLMs) by predominantly using CPU resources while also tapping into GPU capabilities for enhanced processing of particular layers. This adaptability is of great benefit for systems that rely on CPU processing, including those running on Apple hardware. GGUF compresses the typically 16-bit floating-point model weights, optimizing the use of computational resources. The methodology is crafted to simplify the processes of loading and storing models, making it a more efficient, flexible, and user-friendly option for the inference phase of LLMs.


lama-cpp-python
llama-cpp-python is a Python library that features GPU acceleration, compatibility with LangChain, and serves as an API server aligned with OpenAIâ€™s standards.



=== Hardware

LLMs AI models are heavy resource consumers both in terms of memory and CPU. Each of the provided models consumes about 4GiB of memory and requires at least 4 CPUs to run.

So we recommend that a minimum of 12GB of memory and at least 4 CPUs for the Podman machine.

As an additional recommended practice, do nor run more than 3 simultaneous models concurrently.

Please note that this is not relevant for WSL on Windows as the WSL technology the memory and CPU with the host desktop.

=== Technology
Podman AI Lab uses Podman machines to run inference servers for LLM models and AI applications. The AI models can be downloaded, and common formats like GGUF, Pytorch or Tensorflow are supported.

Compatible on Windows, macOS & Linux

=== Software:

Podman Desktop 1.8.0+
Podman 4.9.0+


=== Convert and Quantize Models

AI Lab Recipes' default model server is llamacpp_python, which needs models to be in a *.GGUF format.

However, most models available on huggingface are not provided directly as *.GGUF files. More often they are provided as a set of *.bin or *.safetensor files with some additional metadata produced when the model is trained.

There are of course a number of users on huggingface who provide *.GGUF versions of popular models. But this introduces an unnecessary interim dependency as well as possible security or licensing concerns.

To avoid these concerns and provide users with the maximum freedom of choice for their models, we https://github.com/containers/ai-lab-recipes/tree/main/convert_models[provide a tool] to quickly and easily convert and quantize a model from huggingface into a *.GGUF format for use with our *.GGUF compatible model servers.

=== Model Catalog Example

[YAML]
---
 "models": [
    {
      "id": "hf.instructlab.granite-7b-lab-GGUF",
      "name": "instructlab/granite-7b-lab-GGUF",
      "description": "# InstructLab Granite 7B",
      "hw": "CPU",
      "registry": "Hugging Face",
      "license": "Apache-2.0",
      "url": "https://huggingface.co/instructlab/granite-7b-lab-GGUF/resolve/main/granite-7b-lab-Q4_K_M.gguf",
      "memory": 4080218931,
      "properties": {
        "chatFormat": "openchat"
      },
      "sha256": "6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487",
      "backend": "llama-cpp"
    },
    {
      "id": "hf.instructlab.merlinite-7b-lab-GGUF",
      "name": "instructlab/merlinite-7b-lab-GGUF",
      "description": "# Merlinite 7b - GGUF\n\n4-bit quantized version of [instructlab/merlinite-7b-lab](https://huggingface.co/instructlab/merlinite-7b-lab)",
      "hw": "CPU",
      "registry": "Hugging Face",
      "license": "Apache-2.0",
      "url": "https://huggingface.co/instructlab/merlinite-7b-lab-GGUF/resolve/main/merlinite-7b-lab-Q4_K_M.gguf",
      "memory": 4370129224,
      "properties": {
        "chatFormat": "openchat"
      },
      "sha256": "9ca044d727db34750e1aeb04e3b18c3cf4a8c064a9ac96cf00448c506631d16c",
      "backend": "llama-cpp"
    },
    {
      "id": "hf.TheBloke.mistral-7b-instruct-v0.2.Q4_K_M",
      "name": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
      "description": "# Mistral 7B Instruct v0.2 - GGUF\n- Model creator: [Mistral AI](https://huggingface.co/mistralai)\n- Original model: [Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n\n<!-- description start -->\n## Description\n\nThis repo contains GGUF format model files for [Mistral AI's Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2).\n",
      "hw": "CPU",
      "registry": "Hugging Face",
      "license": "Apache-2.0",
      "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
      "memory": 4370129224,
      "sha256": "3e0039fd0273fcbebb49228943b17831aadd55cbcbf56f0af00499be2040ccf9",
      "backend": "llama-cpp"
    },
    ---
