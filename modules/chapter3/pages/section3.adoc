= Model Services

Once a model has been downloaded, we can start an inference server for the model. This allows testing of the model using a playground environment or to connect applications to the inference server which is exposing a well known OpenAI-compatible API endpoint.

// === Local model serving


// We now apply container technology with the same simplicity and ease of use when it comes to running AI models virtually in a local environment.  These OCI compatible containers can run in any containerized environment making AI Model Portable. 

// To do that, Podman AI Lab provides the ability to spin up local inference servers using llama.cpp within containers. It's now easy to run a model locally, establish an endpoint, and start writing code to wrap new application capabilities around the model.

=== Creating a Model Server

As we experienced during our playground lab, a model service is automatically created as the AI Model endpoint for the playground environment.

However, there will be times, when we don't need the playground, such as experimenting with connecting an existing or new application with the AI Model directly.

To create a model server, go to the Services page and click the Create Model service button. If your environment (podman machine) does not have sufficient resources (memory), we'll get a notification indicating that we need to increase the available memory, which can easily be updated from Podman Desktop's Settings page.

Next, select the desired model you want to serve and if necesssary customize the port you want to use to one already used in your application. 

The list of models displayed in the form will only include models that have been previously downloaded. 

Once the model service has been started, you can open its detail page. This helps you understand the details of the model-service endpoint and explore a number of code snippets that help you learn how to interact with the model service from your application.

image::start-inference-server.gif[width=640]

== LAB - Model Services

==== From the Podman AI Dashboard select *Services* from the navigation menu. 

In the window, any running model service is displayed here with a green indicator.  From this menu, using the icons on the right, we can stop, restart and delete the model service.  If a model is already running from the previous exercise, skip to the next section, otherwise continue:



 . From the top right, select the *New Model Service* button.

 . Select a Model from the Models dropdown (only downloaded models will be populated in the list)

 ..  select the same model from the previous exercise:  *instructlab/granite-7b-lab-GGUF*
 ..  use defaults for the remaining options

 . Select the Create Service button 

 . Once the Service is populated in the window with a green icon, the model endpoint is ready to use

 . Click anywhere along the green icon or the model's container name to open the *Model Service details*.


==== Model Service Details dashboard.

 * First in the window is the *Container section* along with the Model's container name.

 ** the reason we need this name is we can visit the container section of Podman Desktop

 *** to interact directly with the container via a terminal

 *** to view container logs

 *** to view the kubenertes manifest file useed for deployment 

 ***  to inspect the detailed state of the running container

 * Next in the *Model section*, is the model name that was selected to be served, along with tags that indicate Model's license & source repository

 * In the *Server section*, is the local url or inference endpoint for the Model

 ** The CPU interence flag, designates that the Model is Served using CPU only. (no GPU or accelerator)

* The final section is the *Client Code* selector window.

== Client Code 

The _Model Service Details dashboard_ is dedicated to making AI application integration simple.

In the client code section, simply select the programming language from the top right drop down.  

The window will populate with sample code with all the correct information pre-populated to connect to the running model and have it return a response to the question:  What is the captical of France? 

For example is we open a terminal window and paste the generated code from the Curl language, in a few moments the answer will appear.  The format will not be ideal, but it works!

image::curl-example.png[width=800]

---

// Using the dropdown to the right of *client code* , to select the programming language, for some the specifc libraries can be selected to provide example integration code snippets to inference responses from the AI model 

This simplies developers having to search for, research, or learn new technologies to create the integration code base, allowing them to focus on outcomes of the integration.

Use these as a starting point to integrate the model's capabilities into your application's frontent. The API is compatible with the OpenAI format, so you can easily swap between local and hosted models

== GG Example -- Create this

We're working on the GizmoGenie web interface for their Apple mobile application written in swift. integration with the GizmoGobble Mobile App which was development with Swift.

We were able to select the sample integration code for Swift, and add it to the existing code base to receive AI generated ressponses.  Now, we can create simple route queries from Chat application to our Service endopint to interact with the LLM.


Podman Desktop manages the model server container, ensuring high availability and efficient resource utilization. You can monitor its performance and logs through the Podman Desktop dashboard. Since it runs locally, you keep full control of your data and intellectual property.


---

Podman AI Lab enables you to serve the model as a containerized REST endpoint that your code can call, just like any other API. This allows testing of remote connectivity, application integration testing, and evaluation of model's ability to answer test question.





