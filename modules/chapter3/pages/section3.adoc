= Model Services

=== Local model serving


With LLMs, we now apply container technology with the same simplicity and ease of use when it comes to running AI models locally.

To do that, Podman AI Lab provides the ability to spin up local inference servers using llama.cpp within containers. It’s now easy to run a model locally, establish an endpoint, and start writing code to wrap new application capabilities around the model.

To create a model server, go to the Services page and click the Create Model service button. If your environment does not have sufficient memory, you’ll get a notification indicating that you need to increase the available memory, which can easily be updated from Podman Desktop’s Settings page. Here, you can select the desired model you want to serve and define the port you want to use. The list of models displayed in the form will only include models that have been previously downloaded. 

Once the model service has been started, you can open its detail page. This helps you understand the details of the model-service endpoint and see a number of code snippets that help you learn how to interact with the model service from your application; for instance, an application written in Python and using LangChain. See Figure 24.

image::start-inference-server.gif[width=640]

=== LAB - Model Services

From the Podman AI Dashboard select *Services* from the navigation menu. 

in the window, any running model service from the playground is also displayed here as services is the engine that provide the model serving for the playground environment. 

 . From the top right, select the *New Model Service* button.

 . Select a Model from the Models dropdown (only downloaded models will be populated in the list)

 . Selection the Create Service button 

 . Once the Service is populated in the window with a green icon, the Model is ready to use.

 . Click on the green icon or the Model's container name, which is unique identifier number.


This opens the Model Service Details dashboard.

 * First in the window in the container name that is running the Model

 ** the reason we need this name is we can visit the container section of Podman Desktop

 ** to interact directly with the container via a terminal

 ** to view container logs

 ** to view the kubenertes manifest file for deployment 

 **  to inspect the detailed state of the running container.

 * Next is the model name that was selected to be served, along with Model's license & source repository

 * In the Server section is the local url or inference endpoint for the Model

 ** List is the CPU interence, designated that the Model is Served using CPU only. (no GPU or accelerator)

* The final Section is the Client Code selector window.

== Client Code 

Once the inference server is started, the details for the inference server allows you to generate code snippets in various languages to access the model through the inference server.

Using the dropdown to the right of *client code* , the code language and the specifc application types can be selected to show example integration code snippets to inference responses from the AI model 

This simplies developers having to search or create the integration code base, and allows them to focus on the tasks of integrating the solution not building the connection programming code.

Use these as a starting point to integrate the model's capabilities into your application backend. The API is compatible with the OpenAI format, so you can easily swap between local and hosted models. 

Let’s say we’re working on a banking application built with Quarkus, the Kubernetes-native Java stack for developer joy. We can easily integrate the generative AI model into my application by adding the new dependencies for LangChain4j and a connection to my served model to the application.properties. Now, we can create a service to interact with the LLM through a @RegisterAiService annotation, and call that method in other resources (more about Quarkus and Langchain4j here)! Figure 16 depicts this service.
Figure 16: An example Quarkus application with LangChain4j to inference a model.
Podman Desktop manages the model server container, ensuring high availability and efficient resource utilization. You can monitor its performance and logs through the Podman Desktop dashboard. Since it runs locally, you keep full control of your data and intellectual property.


---

Podman AI Lab enables you to serve the model as a containerized REST endpoint that your code can call, just like any other API. From the Services tab, select New Model Service to specify the model and port to expose

