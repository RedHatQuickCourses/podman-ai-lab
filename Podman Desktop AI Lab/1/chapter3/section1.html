<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Model Catalog :: Podman AI Lab</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="section2.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Podman AI Lab</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/changeme/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="Podman Desktop AI Lab" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Podman AI Lab Labs</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Introduction</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section1.html">ABC Corporation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section2.html">Podman</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section3.html">Podman AI Lab</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Installation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Podman Desktop</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section2.html">Podman AI Lab Extention</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Podman AI Lab</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section1.html">Model Catalog</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Playground</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Model Services</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section4.html">Recipe Catalog</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter4/index.html">Advanced Concepts</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter4/section1.html">RAG</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter4/section2.html">InstructLab</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter4/section3.html">Roadmap</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix A</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Podman AI Lab Labs</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Podman AI Lab Labs</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Podman AI Lab Labs</a></li>
    <li><a href="index.html">Podman AI Lab</a></li>
    <li><a href="section1.html">Model Catalog</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Model Catalog</h1>
<div class="sect1">
<h2 id="_ai_models"><a class="anchor" href="#_ai_models"></a>AI Models</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Podman AI Lab provides a curated list of open source AI models and LLMs. Once downloaded, the models are available to be used for AI applications, model services and playgrounds.</p>
</div>
<div class="paragraph">
<p>Check license and required resources. Import your own models.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/download-model.gif" alt="download model" width="640">
</div>
</div>
<div class="paragraph">
<p>Model Catalog Example</p>
</div>
<hr>
<div class="literalblock">
<div class="content">
<pre>"models": [
   {
     "id": "hf.instructlab.granite-7b-lab-GGUF",
     "name": "instructlab/granite-7b-lab-GGUF",
     "description": "# InstructLab Granite 7B",
     "hw": "CPU",
     "registry": "Hugging Face",
     "license": "Apache-2.0",
     "url": "https://huggingface.co/instructlab/granite-7b-lab-GGUF/resolve/main/granite-7b-lab-Q4_K_M.gguf",
     "memory": 4080218931,
     "properties": {
       "chatFormat": "openchat"
     },
     "sha256": "6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487",
     "backend": "llama-cpp"
   },
   {
     "id": "hf.instructlab.merlinite-7b-lab-GGUF",
     "name": "instructlab/merlinite-7b-lab-GGUF",
     "description": "# Merlinite 7b - GGUF\n\n4-bit quantized version of [instructlab/merlinite-7b-lab](https://huggingface.co/instructlab/merlinite-7b-lab)",
     "hw": "CPU",
     "registry": "Hugging Face",
     "license": "Apache-2.0",
     "url": "https://huggingface.co/instructlab/merlinite-7b-lab-GGUF/resolve/main/merlinite-7b-lab-Q4_K_M.gguf",
     "memory": 4370129224,
     "properties": {
       "chatFormat": "openchat"
     },
     "sha256": "9ca044d727db34750e1aeb04e3b18c3cf4a8c064a9ac96cf00448c506631d16c",
     "backend": "llama-cpp"
   },
   {
     "id": "hf.TheBloke.mistral-7b-instruct-v0.2.Q4_K_M",
     "name": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
     "description": "# Mistral 7B Instruct v0.2 - GGUF\n- Model creator: [Mistral AI](https://huggingface.co/mistralai)\n- Original model: [Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n\n&lt;!-- description start --&gt;\n## Description\n\nThis repo contains GGUF format model files for [Mistral AI's Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2).\n",
     "hw": "CPU",
     "registry": "Hugging Face",
     "license": "Apache-2.0",
     "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
     "memory": 4370129224,
     "sha256": "3e0039fd0273fcbebb49228943b17831aadd55cbcbf56f0af00499be2040ccf9",
     "backend": "llama-cpp"
   },
   ---</pre>
</div>
</div>
<div class="sect2">
<h3 id="_from_the_models_section"><a class="anchor" href="#_from_the_models_section"></a>From the Models section</h3>
<div class="paragraph">
<p>You can select to download one or more of the pre-curated models available in the AI Lab.</p>
</div>
<div class="paragraph">
<p>These models all fall under the apache 2.0 license.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
What is Apache 2.0 license in short?
The Apache 2.0 license is a particular type of open-source, permissive software license that ensures that end-users are granted a license to any patent that is covered by the software in question. An Apache 2.0 license ensures the security and availability of safe and powerful open-source software.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_gguf_format"><a class="anchor" href="#_gguf_format"></a>GGUF Format</h3>
<div class="paragraph">
<p>One of the main advantages of GGUF is that it allows users to run LLMs on their CPU. This is particularly beneficial for users who may not own a powerful GPU or who have limited access to GPU resources.</p>
</div>
<div class="paragraph">
<p>"GGUF is a file format for storing models for inference with GGML and executors based on GGML. GGUF is a binary format that is designed for fast loading and saving of models, and for ease of reading. Models are traditionally developed using PyTorch or another framework, and then converted to GGUF for use in GGML."</p>
</div>
<div class="paragraph">
<p>the new GGUF (GPT-Generated Unified Format) framework has been developed to facilitate the operation of Large Language Models (LLMs) by predominantly using CPU resources while also tapping into GPU capabilities for enhanced processing of particular layers. This adaptability is of great benefit for systems that rely on CPU processing, including those running on Apple hardware. GGUF compresses the typically 16-bit floating-point model weights, optimizing the use of computational resources. The methodology is crafted to simplify the processes of loading and storing models, making it a more efficient, flexible, and user-friendly option for the inference phase of LLMs.</p>
</div>
<div class="paragraph">
<p>lama-cpp-python
llama-cpp-python is a Python library that features GPU acceleration, compatibility with LangChain, and serves as an API server aligned with OpenAIâ€™s standards.</p>
</div>
</div>
<div class="sect2">
<h3 id="_hardware"><a class="anchor" href="#_hardware"></a>Hardware</h3>
<div class="paragraph">
<p>LLMs AI models are heavy resource consumers both in terms of memory and CPU. Each of the provided models consumes about 4GiB of memory and requires at least 4 CPUs to run.</p>
</div>
<div class="paragraph">
<p>So we recommend that a minimum of 12GB of memory and at least 4 CPUs for the Podman machine.</p>
</div>
<div class="paragraph">
<p>As an additional recommended practice, do nor run more than 3 simultaneous models concurrently.</p>
</div>
<div class="paragraph">
<p>Please note that this is not relevant for WSL on Windows as the WSL technology the memory and CPU with the host desktop.</p>
</div>
</div>
<div class="sect2">
<h3 id="_technology"><a class="anchor" href="#_technology"></a>Technology</h3>
<div class="paragraph">
<p>Podman AI Lab uses Podman machines to run inference servers for LLM models and AI applications. The AI models can be downloaded, and common formats like GGUF, Pytorch or Tensorflow are supported.</p>
</div>
<div class="paragraph">
<p>Compatible on Windows, macOS &amp; Linux</p>
</div>
</div>
<div class="sect2">
<h3 id="_software"><a class="anchor" href="#_software"></a>Software:</h3>
<div class="paragraph">
<p>Podman Desktop 1.8.0+
Podman 4.9.0+</p>
</div>
</div>
<div class="sect2">
<h3 id="_convert_and_quantize_models"><a class="anchor" href="#_convert_and_quantize_models"></a>Convert and Quantize Models</h3>
<div class="paragraph">
<p>AI Lab Recipes' default model server is llamacpp_python, which needs models to be in a *.GGUF format.</p>
</div>
<div class="paragraph">
<p>However, most models available on huggingface are not provided directly as *.GGUF files. More often they are provided as a set of *.bin or *.safetensor files with some additional metadata produced when the model is trained.</p>
</div>
<div class="paragraph">
<p>There are of course a number of users on huggingface who provide *.GGUF versions of popular models. But this introduces an unnecessary interim dependency as well as possible security or licensing concerns.</p>
</div>
<div class="paragraph">
<p>To avoid these concerns and provide users with the maximum freedom of choice for their models, we <a href="https://github.com/containers/ai-lab-recipes/tree/main/convert_models">provide a tool</a> to quickly and easily convert and quantize a model from huggingface into a *.GGUF format for use with our *.GGUF compatible model servers.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">Podman AI Lab</a></span>
  <span class="next"><a href="section2.html">Playground</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
